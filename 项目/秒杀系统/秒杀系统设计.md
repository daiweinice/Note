# 秒杀系统设计

## 一、发现问题

+ 项目生产环境和开发环境的配置一般会有所不同, 如果需要将开发环境的代码打包并上传到生产环境部署, 就需要修改配置文件, 但是每次重复这种操作其实是很麻烦的。Spring Boot 支持外挂配置文件, 在通过`java -jar`运行项目时, 可以通过`--spring.config.addition-location=/xxx/xxx/application.properties`来指定外挂配置文件, 项目运行时会加载原配置文件和外挂配置文件, 且相同配置项外挂配置文件的优先级更高。

+ 上述案例解决了不同环境不同配置的问题, 但是每次上传代码后还需要输入一长串命令来启动项目很麻烦, 所以还可以编写一个脚本来启动项目, 每次重新上传代码后, 运行脚本即可。

    ```shell
    deploy.sh
    
    nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar xxx.jar --spring.config.addition-location=/xxx/xxx/application.properties
    ```

+ jmeter性能测试相关概念

    + 线程组
    + http请求
    + 结果树
    + 聚合报告

+ jmeter性能测试发现问题

    + 并发线程数上不去
        Spring Boot 有一个`spring-configuration-metadata.json`文件, 里面就定义了各种配置及其默认值, 如 server.port。Tomcat 相关的配置都命名为 server.tomcat.xxx。如`server.tomcat.accept-count`表示最大等待任务数、`server.tomcat.min-spare-threads`表示空闲时开启的线程数(用于应对突发请求)、`server.tomcat.max-connections`、`server.tomcat.max-threads`(默认200)。线程数也不是越多越好, 因为CPU调度线程也需要耗费资源。4核8G一般为800

        ![image-20200807202906978](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200807202906978.png)

        ![image-20200807203113681](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200807203113681.png)

        由于Http是无状态的, 每次建立连接需要花费资源, 所以我们可以使用keepalive功能, 请求时请求头带上keepalive表示客户端希望和服务端建立更长久的连接, 服务端不会立马断开, 而是尝试复用连接。Http之所以设置为无状态就是为了减少客户端占用连接而导致其他客户端无法获得服务的问题。适当使用keepalive可以提高服务性能, 但是若使用不当, 则会造成部分客户端一直占有连接。所以keepalive需要进行响应的配置优化, 但这些配置spring boot没有提供, 需要我们定制化内嵌tomcat配置。详情查看代码。

        ![image-20200807204023208](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200807204023208.png)

    + 响应时间变长, TPS上不去

        ![image-20200807204751064](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200807204751064.png)


## 二、分布式扩展

分布式扩展是提高服务性能的一个方法。

### nginx反向代理、负载均衡、动静分离

动静分离

![image-20200809201557276](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809201557276.png)

反向代理

![image-20200809202043786](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809202043786.png)

tomcat access log 用于记录请求来源和响应时间等信息, 虽然会占用系统资源但是它是异步处理的不会占用主线程, 所以一般建议开启。

![image-20200809202558044](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809202558044.png)

nginx反向代理与被代理的服务器默认没有使用keepalive, 需要手动设置

![image-20200809204816984](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809204816984.png)

![image-20200809205303043](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809205303043.png)

### nginx高性能原因

+ epoll多路复用

    ![image-20200809210445654](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809210445654.png)

+ master-worker模型实现无缝重启

    master进程用于管理worker进程, worker进程用于处理客户端连接

    重启时master不会挂, 而worker会进行更新

    master是worker的父进程, 所以master可以获得worker中的所有数据。

    每一个worker都是单线程的

    ![image-20200809211247322](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809211247322.png)

+ 协程机制

    ![image-20200809211559535](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200809211559535.png)

    我们的每个worker都是单线程的, 那么如何处理成千上万的请求呢, 如果是js的方式就是异步化, 但是异步化难以保证执行顺序(除非回调嵌套)。nginx开发了一个协程机制, 协程是比线程更小的概念, 协程程序遇到阻塞会归还执行权, 交给不阻塞的线程执行, 这以为这协程运行是顺序的

    lua的操作也是基于协程机制的操作。所以可以用lua来实现nginx高性能



### 分布式session管理

sessionid的传输一般有两种, 一种是cookie传输sessionid, 一种是token传输sessionid。但是上述两种方法在分布式环境下是存在一定问题的, 因为session存储是基于tomcat, 不同的服务器之间的session是无法共享的, 所以可以将session移动到redis中, 且无需开启持久化。

导入jar包`spring-boot-starter-session`、`spring-session-data-redis`

![image-20200810192229624](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200810192229624.png)

上述代码就完成了配置, 此时就实现了基于cookie的session共享

要存入session的对象需要实现serilizable接口, 这是redis默认的序列化方式。当然我们也可以配置为json的方式



基于token的session共享

![image-20200810193949582](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200810193949582.png)

前端将token保存再localstorage中, 虽然localstorage是无过期时间的, 但是后端设置了过期时间, 所以可以存放在localstorage中。localstorage与cookie相比, 突破了cookie4k的容量限制, 且有的设备或浏览器是禁止cookie的, 所以推荐使用localstorage来代替cookie

由于localstorage没有过期时间需要我们手动删除, 所以我们可以将数据以json字符串的方式保存再token中, 同时加上一个expire字段, 用于记录过期时间, 页面加载时先判断token是否过期, 如果过期了则删除token



## 三、查询优化技术之多级缓存

![image-20200814183930694](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814183930694.png)

### redis

注意key和value的序列化方式

使json字符串中添加类信息

<img src="https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814190824320.png" alt="image-20200814190824320" />

### 本地缓存

![image-20200814191523971](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814191523971.png)



![image-20200814192149406](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814192149406.png)

使用HashMap做缓存有很多问题, 比如大小无法控制, ConcurrentHashMap在写时会对并发读产生影响。

google推出了guava cache, 可以解决这个问题, 它可配置lru策略(Least Recently Used), 见最近最少使用

本地缓存在redis之前, 即先从本地缓存中获取, 本地缓存中没有则从redis中取, redis中没有则从数据库中取, 然后再存入本地缓存和redis

使用了本地缓存后性能提升很多, tps直接变成了3000多。但是缺点就是当数据库更新时, 我们没有一个很好的方法取更新本地缓存。

### Nginx缓存

![image-20200814193547735](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814193547735.png)

key在缓存中, value在文件中

![image-20200814193947554](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814193947554.png)

虽然nginx缓存使得请求不用在转发到服务器, 但是nginx缓存本质上还是从磁盘文件中读取数据, 所以使用了nginx缓存性能反而没有只用本地缓存高。当然可以使用lua脚本进行优化。

![image-20200814194747281](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814194747281.png)

![image-20200814194824472](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814194824472.png)

![image-20200814195106298](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814195106298.png)

![image-20200814195311549](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814195311549.png)

![image-20200814195719340](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814195719340.png)

![image-20200814195739272](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814195739272.png)

![image-20200814200113522](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814200113522.png)

![image-20200814200121931](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814200121931.png)

一般经常使用content_by_lua挂载点, 所以可以使用lua脚本来实现请求的业务逻辑处理

![image-20200814200310417](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814200310417.png)

![image-20200814200749017](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814200749017.png)

![image-20200814200843242](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814200843242.png)

![image-20200814201858328](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814201858328.png)

使用nginx lua优化后tps和本地缓存差不多, 不过虽然nginx减少向服务端的一次访问, 但是把请求都集中在了nginx服务器上, nginx服务器压力大

我们还可以使用nginx从redis中读取数据

![image-20200814203010131](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200814203010131.png)



## 四、页面静态化

![image-20200815191422192](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815191422192.png)

![image-20200815192128258](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815192128258.png)

![image-20200815192331327](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815192331327.png)

https://segmentfault.com/a/1190000021560126

![image-20200815192900533](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815192900533.png)

![image-20200815192916345](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815192916345.png)

![image-20200815193413601](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815193413601.png)

控制台优先级 > 源站配置

![image-20200815194547635](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815194547635.png)

![image-20200815194605316](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815194605316.png)

![image-20200815194634406](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815194634406.png)

![image-20200815195027347](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815195027347.png)

PhantomJS是一款无头浏览器, 可以借助其模拟webkit.js的执行

全面静态化的思路就是利用PhantomJS爬取到网页的内容(html文件), 然后将网页推送到cdn上, 这样下一次相同的请求到达时, 就是直接访问的带有数据信息的静态网页, 而不需要重新发送ajax请求获取信息。但是上述实现需要注意静态网页中虽然已有数据, 但是还是有ajax的代码, 所以还是会出现ajax请求的情况, 所以需要防止重复初始化。

![image-20200815200754757](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815200754757.png)

目前PhantomJS已停止维护, 谷歌自己也推出了无头版本, 所以建议使用谷歌无头浏览器。

防止页面初始化, 我们可以定义一个隐藏域`<input type="hidden" id="isInit" value="0">`来标志网页是初次加载, 还是已经是加载过的静态网页, 如果是初次加载再执行ajax请求

![image-20200815201859878](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200815201859878.png)



## 五、交易优化之缓存库存

在最原始的项目中, 每一次数据操作都需要访问数据库, 所以性能较低。所以我们可以把相关数据放到缓存中。

![image-20200816163407149](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816163407149.png)

### 验证优化

在获取秒杀商品时需要验证商品信息和用户信息, 先从缓存中读商品信息和user信息, 若没有则从数据库中读并同步到缓存中

### 库存优化

将库存信息放到缓存中。

商品若需要紧急下架, 则先删除库存缓存和商品缓存, 然后将数据库中对应商品修改为下架状态, 这样用户在进行获取秒杀商品信息时都不会验证通过。

### 扣减商品

![image-20200816164907830](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816164907830.png)

但是缓存扣减库存, 数据库记录与缓存数据是不一致的。所以我们可以在缓存减库存后, 发送异步消息扣减数据库的库存。

### 分布式事务

![image-20200816170231231](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816170231231.png)

分布式环境涉及事务一致性, 但是CAP我们只能保证其二, 所以我们不保证强一致性, 而是保证最终一致性即可。即当缓存扣减成功时,缓存的库存为99, 而数据库此时还是100, 但是当异步消息被处理后, 数据库也变成99, 这就是最终一致性

![image-20200816171535486](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816171535486.png)

上述代码还有很多问题, 如扣减库存是成功了, 异步消息也成功发送了并处理了, 但是后续保存订单的时候出现了问题, 此种情况虽然不会产生超卖, 但是会产生少卖。

解决上述情况, 我们可以在一整套流程, 即扣库存、保存订单等操作结束后, 再发送异步消息。但是还是有问题, 当事务的方法返回后, 才会提交事务, 如果库存扣减、订单保存等都完成了, 异步消息也发送了, 但是提交事务由于网络问题出错了, 则库存还是多扣减了, 所以spring的事务给我们提供了一个当事务完成时才执行对应方法的机制

![image-20200816193339165](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816193339165.png)

但是上述代码, 如果发送异步消息失败, 则无法回滚。所以我们需要MQ的事务机制

再MQ事务中去保存订单。

但是若保存订单长时间没有结果, 我们没有办法判断当前订单是否已经生成, 问题的本质是我们缺少操作性数据, 即操作记录。操作记录保存再数据库中, 且有3个状态, 1, 初始状态 2. 成功 3, 回滚

![image-20200816200903802](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816200903802.png)

不同公司的高可用策略是不同的, 有的公司可能会选择宁可超卖不可少卖

![image-20200816201233145](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200816201233145.png)

### 库存售罄优化

售罄后再redis添加对应商品售罄标识, 后面的请求发现售罄就不会执行后面的逻辑



## 削峰限流

上面的实现还存在问题, 如秒杀接口会被脚本不断的访问, 同时即使在没有开始秒杀时, 秒杀接口也会被暴露, 会被恶意访问。

### 秒杀令牌

![image-20200817081241630](https://blog-1258617239.cos.ap-chengdu.myqcloud.com/blog_images/image-20200817081241630.png)

用户在下单前会先进行商品验证、用户信息验证等验证, 验证成功后会生成秒杀token, 用户需要带上秒杀token才能进行下单操作。

秒杀令牌还有缺陷, 当有一亿个用户参与秒杀就会生成1亿个令牌, 然而只有极少数能秒杀成功。所以引入秒杀大闸。在创建秒杀活动时, 根据库存数设置一个令牌数量, 如5*库存数, 每次有用户获取到令牌则令牌数-1, 当令牌数小于0时就不再生成令牌

但是令牌大闸还是会有大量流量涌入的问题, 且多库存、多商品等令牌限制能力弱

创建一个线程池, 利用线程池20个并发请求的限制, 实现并发控制。里面使用callable接口, 返回future判断下单是否成功。但是上述方法只局限于本地限流, 若在分布式环境下, 则需要将队列设置到外部redis内, 当然使用redis又多了一次网络请求操作, 且有单点故障问题。所以一般推荐使用本地线程池进行并发控制。

## 防刷限流

### 前端验证码

生成秒杀令牌之前先请求验证码

验证码的作用是为了将用户的请求分散

前端如何显示验证码? `<img src="验证码URL">`

### 限流

TPS每秒事务数, 即一个请求从发起到服务端响应

QPS每秒查询的数量



令牌桶算法: 令牌数初始有N个, 一个请求减一个, 然后每一秒重置令牌数为10

,漏桶算法: 客户端加一滴水, 桶10滴水满, 每秒以一滴水流出

令牌桶算法限制了并发的请求个数, 而漏桶算法是为了平滑请求处理效率, 不能应对突发流量。

所以一般使用令牌桶算法



限流有两种方式, 接口限流和总维度限流, 又分为单机限流和集群限流, 负载均衡前提下单机限流更好

Guava提供了令牌桶限流API, RateLimiter





## X、其他

+ 一般要求服务器的容量是无限的, 我们可以使用NAS来存储文件。
+ 许多的html文件都有发送ajax请求的代码, 但是如果我们变更了服务器地址, 则需要挨个替换很麻烦。我们可以编写一个js, 定义一个变量存储服务器地址, 其他地方使用该变量即可, 这样可以一处修改, 多处生效。
+ Nginx安装需要手动指定模块、配置相关参数, 非常麻烦,  所以推荐使用openresty。 OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。OpenReaty集成了nginx, 如果我们想使用自己的nginx, 直接打包替换掉openresty的nginx即可

